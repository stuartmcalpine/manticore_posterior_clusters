{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b87030-a9fc-437e-95f1-2bff7d56a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from backend.config_loader import load_config\n",
    "from backend.utils import *\n",
    "from pymanticore.analysis.matplotlib import get_mplstyle_path, ManticoreColors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435f193a-4848-488b-8042-e1447cee7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration Loading\n",
    "config = load_config()\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Base directory: {config.global_config.basedir}\")\n",
    "print(f\"Output directory: {config.global_config.output_dir}\")\n",
    "print(f\"Observer coordinates: {config.global_config.observer_coords}\")\n",
    "print(f\"Mode1 - eps: {config.mode1.eps}, min_samples: {config.mode1.min_samples}\")\n",
    "print(f\"Mode2 - target_snapshot: {config.mode2.target_snapshot}, min_cluster_size: {config.mode2.min_cluster_size}\")\n",
    "\n",
    "mnras_style = \"mnras\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb5681-015d-488f-9462-76c59e2d10a1",
   "metadata": {},
   "source": [
    "# Investigate the `eps` and `min_samples` dependence\n",
    "\n",
    "Load Cluster Data for Different Eps and Min_Samples Values with Position and Mass Std Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7d9138-a2b7-4501-9300-14108864b3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter ranges\n",
    "eps_values = [1.5, 1.75, 2.0, 2.5, 3.0, 3.5]\n",
    "min_samples_values = [5, 7, 9]\n",
    "\n",
    "# Storage for results\n",
    "eps_min_samples_results = {}\n",
    "\n",
    "for min_samples in min_samples_values:\n",
    "   eps_min_samples_results[min_samples] = {}\n",
    "   \n",
    "   for eps in eps_values:\n",
    "       # Convert eps to filename format (2.5 -> \"2p5\")\n",
    "       eps_str = str(eps).replace('.', 'p')\n",
    "       filename = f\"clusters_eps_{eps_str}_min_samples_{min_samples}.h5\"\n",
    "       \n",
    "       try:\n",
    "           clusters, cluster_metadata = load_clusters_from_hdf5(config.global_config.output_dir, filename=filename)\n",
    "           \n",
    "           # Calculate basic metrics\n",
    "           n_clusters = len(clusters)\n",
    "           cluster_sizes = [c['cluster_size'] for c in clusters]\n",
    "           mean_cluster_size = np.mean(cluster_sizes) if cluster_sizes else 0\n",
    "           max_cluster_size = max(cluster_sizes) if cluster_sizes else 0\n",
    "           large_clusters = len([s for s in cluster_sizes if s >= 40])\n",
    "           \n",
    "           # Filter to only large clusters (>= 40 members) for std calculations\n",
    "           large_cluster_data = [c for c in clusters if c['cluster_size'] >= 40]\n",
    "           \n",
    "           # Calculate position std statistics for large clusters only\n",
    "           position_stds = []\n",
    "           log_mass_stds = []\n",
    "           mass_stds = []\n",
    "           large_cluster_sizes = []\n",
    "           \n",
    "           for cluster in large_cluster_data:\n",
    "               # Position std magnitude (3D vector -> scalar)\n",
    "               pos_std = cluster.get('position_std', np.array([0, 0, 0]))\n",
    "               position_stds.append(np.linalg.norm(pos_std))\n",
    "\n",
    "               # Linear mass std for comparison\n",
    "               log_mass_stds.append(cluster.get('log10_m200_mass_std', 0))\n",
    "               mass_stds.append(cluster.get('m200_mass_std', 0))\n",
    "               large_cluster_sizes.append(cluster['cluster_size'])\n",
    "           \n",
    "           # Summary statistics for large clusters only\n",
    "           mean_position_std = np.mean(position_stds) if position_stds else 0# Cell: \n",
    "\n",
    "           mean_log_mass_std = np.mean(log_mass_stds) if log_mass_stds else 0\n",
    "           mean_mass_std = np.mean(mass_stds) if mass_stds else 0\n",
    "           \n",
    "           # Size-weighted statistics for large clusters\n",
    "           if large_cluster_sizes and position_stds:\n",
    "               weights = np.array(large_cluster_sizes)\n",
    "               weighted_mean_position_std = np.average(position_stds, weights=weights)\n",
    "               weighted_mean_log_mass_std = np.average(log_mass_stds, weights=weights)\n",
    "           else:\n",
    "               weighted_mean_position_std = 0\n",
    "               weighted_mean_log_mass_std = 0\n",
    "           \n",
    "           eps_min_samples_results[min_samples][eps] = {\n",
    "               'n_clusters': n_clusters,\n",
    "               'mean_size': mean_cluster_size,\n",
    "               'max_size': max_cluster_size,\n",
    "               'large_clusters': large_clusters,\n",
    "               'cluster_sizes': cluster_sizes,\n",
    "               'mean_position_std': mean_position_std,\n",
    "               'mean_log_mass_std': mean_log_mass_std,\n",
    "               'mean_mass_std': mean_mass_std,\n",
    "               'weighted_mean_position_std': weighted_mean_position_std,\n",
    "               'weighted_mean_log_mass_std': weighted_mean_log_mass_std,\n",
    "               'position_stds': position_stds,\n",
    "               'log_mass_stds': log_mass_stds,\n",
    "               'mass_stds': mass_stds,\n",
    "               'n_large_clusters': len(large_cluster_data),\n",
    "               'clusters': clusters,\n",
    "               'metadata': cluster_metadata\n",
    "           }\n",
    "           \n",
    "           print(f\"min_samples={min_samples}, eps={eps}: {n_clusters} clusters, \"\n",
    "                 f\"large={large_clusters}, pos_std={mean_position_std:.2f}, \"\n",
    "                 f\"log_mass_std={mean_log_mass_std:.3f}\")\n",
    "           \n",
    "       except FileNotFoundError:\n",
    "           print(f\"File not found for min_samples={min_samples}, eps={eps}: {filename}\")\n",
    "           eps_min_samples_results[min_samples][eps] = None\n",
    "       except Exception as e:\n",
    "           print(f\"Error loading min_samples={min_samples}, eps={eps}: {e}\")\n",
    "           eps_min_samples_results[min_samples][eps] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e0cc9-8c63-47f8-be4d-f1dd5b5f6b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Enhanced Parameter Analysis Plots Including Position and Mass Std (Large Clusters Only)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "markers = ['o', 's', '^', 'D', 'v']\n",
    "\n",
    "# Extract data for all min_samples values\n",
    "plot_data = {}\n",
    "for i, min_samples in enumerate(min_samples_values):\n",
    "    plot_data[min_samples] = {\n",
    "        'valid_eps': [],\n",
    "        'n_clusters': [],\n",
    "        'mean_sizes': [],\n",
    "        'large_clusters': [],\n",
    "        'mean_position_stds': [],\n",
    "        'mean_log_mass_stds': [],\n",
    "        'weighted_position_stds': [],\n",
    "        'weighted_log_mass_stds': []\n",
    "    }\n",
    "    \n",
    "    for eps in eps_values:\n",
    "        if eps_min_samples_results[min_samples].get(eps) is not None:\n",
    "            data = eps_min_samples_results[min_samples][eps]\n",
    "            plot_data[min_samples]['valid_eps'].append(eps)\n",
    "            plot_data[min_samples]['n_clusters'].append(data['n_clusters'])\n",
    "            plot_data[min_samples]['mean_sizes'].append(data['mean_size'])\n",
    "            plot_data[min_samples]['large_clusters'].append(data['large_clusters'])\n",
    "            plot_data[min_samples]['mean_position_stds'].append(data['mean_position_std'])\n",
    "            plot_data[min_samples]['mean_log_mass_stds'].append(data['mean_log_mass_std'])\n",
    "            plot_data[min_samples]['weighted_position_stds'].append(data['weighted_mean_position_std'])\n",
    "            plot_data[min_samples]['weighted_log_mass_stds'].append(data['weighted_mean_log_mass_std'])\n",
    "\n",
    "# Plot 1: Total number of clusters vs eps\n",
    "for i, min_samples in enumerate(min_samples_values):\n",
    "    if len(plot_data[min_samples]['valid_eps']) > 0:\n",
    "        axes[0].plot(plot_data[min_samples]['valid_eps'], plot_data[min_samples]['n_clusters'], \n",
    "                    color=colors[i], marker=markers[i], linewidth=2, markersize=8, \n",
    "                    label=f'min_samples={min_samples}')\n",
    "axes[0].set_xlabel('Eps (Mpc)')\n",
    "axes[0].set_ylabel('Number of Clusters')\n",
    "axes[0].set_title('Total Clusters vs Eps')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Mean cluster size vs eps\n",
    "for i, min_samples in enumerate(min_samples_values):\n",
    "    if len(plot_data[min_samples]['valid_eps']) > 0:\n",
    "        axes[1].plot(plot_data[min_samples]['valid_eps'], plot_data[min_samples]['mean_sizes'], \n",
    "                    color=colors[i], marker=markers[i], linewidth=2, markersize=8, \n",
    "                    label=f'min_samples={min_samples}')\n",
    "axes[1].set_xlabel('Eps (Mpc)')\n",
    "axes[1].set_ylabel('Mean Cluster Size')\n",
    "axes[1].set_title('Mean Cluster Size vs Eps')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot 3: Large clusters (size >= 40) vs eps\n",
    "for i, min_samples in enumerate(min_samples_values):\n",
    "    if len(plot_data[min_samples]['valid_eps']) > 0:\n",
    "        axes[2].plot(plot_data[min_samples]['valid_eps'], plot_data[min_samples]['large_clusters'], \n",
    "                    color=colors[i], marker=markers[i], linewidth=2, markersize=8, \n",
    "                    label=f'min_samples={min_samples}')\n",
    "axes[2].set_xlabel('Eps (Mpc)')\n",
    "axes[2].set_ylabel('Number of Large Clusters (≥40)')\n",
    "axes[2].set_title('Large Clusters vs Eps')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].legend()\n",
    "\n",
    "# Plot 4: Mean position std vs eps (large clusters only)\n",
    "for i, min_samples in enumerate(min_samples_values):\n",
    "    if len(plot_data[min_samples]['valid_eps']) > 0:\n",
    "        axes[3].plot(plot_data[min_samples]['valid_eps'], plot_data[min_samples]['mean_position_stds'], \n",
    "                    color=colors[i], marker=markers[i], linewidth=2, markersize=8, \n",
    "                    label=f'min_samples={min_samples}')\n",
    "axes[3].set_xlabel('Eps (Mpc)')\n",
    "axes[3].set_ylabel('Mean Position Std (Mpc)')\n",
    "axes[3].set_title('Mean Position Std vs Eps (Large Clusters ≥40)')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "axes[3].legend()\n",
    "\n",
    "# Plot 5: Mean log mass std vs eps (large clusters only)\n",
    "for i, min_samples in enumerate(min_samples_values):\n",
    "    if len(plot_data[min_samples]['valid_eps']) > 0:\n",
    "        axes[4].plot(plot_data[min_samples]['valid_eps'], plot_data[min_samples]['mean_log_mass_stds'], \n",
    "                    color=colors[i], marker=markers[i], linewidth=2, markersize=8, \n",
    "                    label=f'min_samples={min_samples}')\n",
    "axes[4].set_xlabel('Eps (Mpc)')\n",
    "axes[4].set_ylabel('Mean Log Mass Std (dex)')\n",
    "axes[4].set_title('Mean Log Mass Std vs Eps (Large Clusters ≥40)')\n",
    "axes[4].grid(True, alpha=0.3)\n",
    "axes[4].legend()\n",
    "\n",
    "# Plot 6: Size-weighted position std vs eps (large clusters only)\n",
    "for i, min_samples in enumerate(min_samples_values):\n",
    "    if len(plot_data[min_samples]['valid_eps']) > 0:\n",
    "        axes[5].plot(plot_data[min_samples]['valid_eps'], plot_data[min_samples]['weighted_position_stds'], \n",
    "                    color=colors[i], marker=markers[i], linewidth=2, markersize=8, \n",
    "                    label=f'min_samples={min_samples}')\n",
    "axes[5].set_xlabel('Eps (Mpc)')\n",
    "axes[5].set_ylabel('Weighted Mean Position Std (Mpc)')\n",
    "axes[5].set_title('Size-Weighted Position Std vs Eps (Large Clusters ≥40)')\n",
    "axes[5].grid(True, alpha=0.3)\n",
    "axes[5].legend()\n",
    "\n",
    "# Plot 7: Size-weighted log mass std vs eps (large clusters only)\n",
    "for i, min_samples in enumerate(min_samples_values):\n",
    "    if len(plot_data[min_samples]['valid_eps']) > 0:\n",
    "        axes[6].plot(plot_data[min_samples]['valid_eps'], plot_data[min_samples]['weighted_log_mass_stds'], \n",
    "                    color=colors[i], marker=markers[i], linewidth=2, markersize=8, \n",
    "                    label=f'min_samples={min_samples}')\n",
    "axes[6].set_xlabel('Eps (Mpc)')\n",
    "axes[6].set_ylabel('Weighted Mean Log Mass Std (dex)')\n",
    "axes[6].set_title('Size-Weighted Log Mass Std vs Eps (Large Clusters ≥40)')\n",
    "axes[6].grid(True, alpha=0.3)\n",
    "axes[6].legend()\n",
    "\n",
    "# Plot 8: Position std vs log mass std scatter (large clusters only)\n",
    "for i, min_samples in enumerate(min_samples_values):\n",
    "    for eps in eps_values:\n",
    "        data = eps_min_samples_results[min_samples].get(eps)\n",
    "        if data is not None and data['n_large_clusters'] > 0:\n",
    "            axes[7].scatter(data['mean_position_std'], data['mean_log_mass_std'], \n",
    "                          color=colors[i], s=50, alpha=0.7, \n",
    "                          label=f'min_samples={min_samples}' if eps == eps_values[0] else \"\")\n",
    "axes[7].set_xlabel('Mean Position Std (Mpc)')\n",
    "axes[7].set_ylabel('Mean Log Mass Std (dex)')\n",
    "axes[7].set_title('Position Std vs Log Mass Std (Large Clusters ≥40)')\n",
    "axes[7].grid(True, alpha=0.3)\n",
    "axes[7].legend()\n",
    "\n",
    "# Plot 9: Summary table\n",
    "axes[8].axis('off')\n",
    "table_text = \"Parameter Summary (Large Clusters ≥40)\\n\\n\"\n",
    "table_text += f\"{'MinSamp':<8} {'Eps':<6} {'N_Large':<8} {'Pos_Std':<8} {'LogM_Std':<10}\\n\"\n",
    "table_text += \"-\" * 50 + \"\\n\"\n",
    "\n",
    "for min_samples in min_samples_values:\n",
    "    for eps in eps_values:\n",
    "        data = eps_min_samples_results[min_samples].get(eps)\n",
    "        if data is not None:\n",
    "            table_text += f\"{min_samples:<8} {eps:<6} {data['large_clusters']:<8} {data['mean_position_std']:<8.2f} {data['mean_log_mass_std']:<10.3f}\\n\"\n",
    "\n",
    "axes[8].text(0.05, 0.95, table_text, transform=axes[8].transAxes, fontsize=9, \n",
    "             verticalalignment='top', fontfamily='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed summary\n",
    "print(f\"\\n{'MinSamp':<8} {'Eps':<6} {'N_Clust':<8} {'N_Large':<8} {'Pos_Std':<8} {'LogM_Std':<10} {'Wtd_Pos':<8} {'Wtd_LogM':<10}\")\n",
    "print(\"-\" * 90)\n",
    "for min_samples in min_samples_values:\n",
    "    for eps in eps_values:\n",
    "        data = eps_min_samples_results[min_samples].get(eps)\n",
    "        if data is not None:\n",
    "            print(f\"{min_samples:<8} {eps:<6} {data['n_clusters']:<8} {data['large_clusters']:<8} \"\n",
    "                  f\"{data['mean_position_std']:<8.2f} {data['mean_log_mass_std']:<10.3f} \"\n",
    "                  f\"{data['weighted_mean_position_std']:<8.2f} {data['weighted_mean_log_mass_std']:<10.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b50489-25f6-4776-b396-b3fb1529d0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Cell: Multi-Parameter Cluster Comparison Around Reference Location (Optimized)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from backend.utils import load_clusters_from_hdf5, find_clusters_in_window, load_single_cluster_members\n",
    "\n",
    "# Configuration  \n",
    "reference_eps = 2.5\n",
    "reference_min_samples = 7\n",
    "nth_most_massive = 6  # Select the Nth most massive cluster (1-indexed)\n",
    "window_size = 20.0  # Mpc radius around centroid\n",
    "\n",
    "# Load reference cluster data (minimal load to get centroids and masses only)\n",
    "ref_eps_str = str(reference_eps).replace('.', 'p')\n",
    "ref_filename = f\"clusters_eps_{ref_eps_str}_min_samples_{reference_min_samples}.h5\"\n",
    "ref_clusters, ref_metadata = load_clusters_from_hdf5(config.global_config.output_dir, filename=ref_filename, minimal=True)\n",
    "\n",
    "# Sort reference clusters by mean mass and select the Nth most massive\n",
    "ref_clusters_sorted = sorted(ref_clusters, key=lambda x: x['mean_m200_mass'], reverse=True)\n",
    "if nth_most_massive > len(ref_clusters_sorted):\n",
    "    print(f\"Only {len(ref_clusters_sorted)} clusters available, selecting the most massive\")\n",
    "    reference_cluster = ref_clusters_sorted[0]\n",
    "else:\n",
    "    reference_cluster = ref_clusters_sorted[nth_most_massive - 1]\n",
    "\n",
    "reference_centroid = reference_cluster['mean_position']\n",
    "reference_centroid = [460, 482, 470]\n",
    "print(f\"Reference cluster: ID {reference_cluster['cluster_id']}, mass {reference_cluster['mean_m200_mass']:.2e}\")\n",
    "print(f\"Reference centroid: [{reference_centroid[0]:.1f}, {reference_centroid[1]:.1f}, {reference_centroid[2]:.1f}]\")\n",
    "\n",
    "# Define plot window\n",
    "x_min, x_max = reference_centroid[0] - window_size, reference_centroid[0] + window_size\n",
    "y_min, y_max = reference_centroid[1] - window_size, reference_centroid[1] + window_size\n",
    "\n",
    "# Calculate subplot grid\n",
    "n_eps = len(eps_values)\n",
    "n_min_samples = len(min_samples_values)\n",
    "\n",
    "# Create figure with appropriate size\n",
    "fig_width = n_eps * 4\n",
    "fig_height = n_min_samples * 4\n",
    "fig, axes = plt.subplots(n_min_samples, n_eps, figsize=(fig_width, fig_height))\n",
    "if n_min_samples == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "if n_eps == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "# Plot for each parameter combination\n",
    "for row_idx, min_samples in enumerate(min_samples_values):\n",
    "    for col_idx, eps in enumerate(eps_values):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        \n",
    "        # Generate filename for this parameter set\n",
    "        eps_str = str(eps).replace('.', 'p')\n",
    "        filename = f\"clusters_eps_{eps_str}_min_samples_{min_samples}.h5\"\n",
    "        \n",
    "        try:\n",
    "            # Find clusters in window (metadata only)\n",
    "            window_cluster_ids = find_clusters_in_window(config.global_config.output_dir, filename, \n",
    "                                                        reference_centroid, window_size)\n",
    "            \n",
    "            if len(window_cluster_ids) == 0:\n",
    "                ax.text(0.5, 0.5, f'No clusters\\nin window', \n",
    "                       transform=ax.transAxes, ha='center', va='center')\n",
    "            else:\n",
    "                # Load individual cluster members\n",
    "                colors = plt.cm.tab20(np.linspace(0, 1, 20))\n",
    "                if len(window_cluster_ids) > 20:\n",
    "                    colors = plt.cm.gist_ncar(np.linspace(0, 1, len(window_cluster_ids)))\n",
    "                \n",
    "                for i, cluster_id in enumerate(window_cluster_ids):\n",
    "                    cluster_data = load_single_cluster_members(config.global_config.output_dir, \n",
    "                                                             filename, cluster_id)\n",
    "                    \n",
    "                    if cluster_data is not None:\n",
    "                        positions = cluster_data['member_data']['BoundSubhalo/CentreOfMass']\n",
    "                        \n",
    "                        # Filter positions to window\n",
    "                        mask = ((positions[:, 0] >= x_min) & (positions[:, 0] <= x_max) & \n",
    "                               (positions[:, 1] >= y_min) & (positions[:, 1] <= y_max))\n",
    "                        window_positions = positions[mask]\n",
    "                        \n",
    "                        if len(window_positions) > 0:\n",
    "                            color = colors[i % len(colors)]\n",
    "                            ax.scatter(window_positions[:, 0], window_positions[:, 1], \n",
    "                                     c=[color], s=30, alpha=0.7, \n",
    "                                     label=f'ID {cluster_id} (n={cluster_data[\"cluster_size\"]})')\n",
    "            \n",
    "            # Mark reference centroid\n",
    "            ax.scatter(reference_centroid[0], reference_centroid[1], \n",
    "                      c='black', s=200, marker='*', \n",
    "                      edgecolors='white', linewidth=2, zorder=10)\n",
    "            \n",
    "            # Add circle showing window boundary\n",
    "            circle = plt.Circle((reference_centroid[0], reference_centroid[1]), \n",
    "                               window_size, fill=False, color='black', \n",
    "                               linestyle='--', linewidth=1, alpha=0.8)\n",
    "            ax.add_patch(circle)\n",
    "            \n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_title(f'eps={eps}, min_samples={min_samples}\\n{len(window_cluster_ids)} clusters')\n",
    "            \n",
    "            if row_idx == n_min_samples - 1:\n",
    "                ax.set_xlabel('X (Mpc)')\n",
    "            if col_idx == 0:\n",
    "                ax.set_ylabel('Y (Mpc)')\n",
    "                \n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, f'Error loading\\neps={eps}\\nmin_samples={min_samples}', \n",
    "                   transform=ax.transAxes, ha='center', va='center')\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "\n",
    "plt.suptitle(f'Cluster Comparison Around Reference Location\\n'\n",
    "             f'Reference: eps={reference_eps}, min_samples={reference_min_samples}, '\n",
    "             f'{nth_most_massive}{\"st\" if nth_most_massive==1 else \"nd\" if nth_most_massive==2 else \"rd\" if nth_most_massive==3 else \"th\"} most massive cluster\\n'\n",
    "             f'Window: ±{window_size} Mpc around [{reference_centroid[0]:.1f}, {reference_centroid[1]:.1f}, {reference_centroid[2]:.1f}]',\n",
    "             fontsize=16, y=0.98)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.92)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33edc485-34f4-49fe-a316-d20a869a2847",
   "metadata": {},
   "source": [
    "# Cluster occurence in random simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ea835-9f06-40b4-bde2-bdb858712fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empirical_null_analysis.py\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load cluster sizes from HDF5 file\n",
    "with h5py.File('output/random_control_clusters.h5', 'r') as f:\n",
    "    cluster_sizes = []\n",
    "    clusters_grp = f['clusters']\n",
    "    for cluster_name in clusters_grp.keys():\n",
    "        cluster_sizes.append(clusters_grp[cluster_name].attrs['cluster_size'])\n",
    "\n",
    "cluster_sizes = np.array(cluster_sizes)\n",
    "\n",
    "# Empirical survival function: P(size >= n)\n",
    "def empirical_survival(n):\n",
    "    return np.mean(cluster_sizes >= n)\n",
    "\n",
    "# Calculate significance thresholds\n",
    "size_range = np.arange(3, max(cluster_sizes) + 1)\n",
    "survival_probs = [empirical_survival(n) for n in size_range]\n",
    "\n",
    "# Find thresholds for different significance levels\n",
    "alpha_levels = [0.1, 0.05, 0.01, 0.001]\n",
    "thresholds = {}\n",
    "\n",
    "for alpha in alpha_levels:\n",
    "    threshold_candidates = [n for n, p in zip(size_range, survival_probs) if p <= alpha]\n",
    "    if threshold_candidates:\n",
    "        thresholds[alpha] = min(threshold_candidates)\n",
    "    else:\n",
    "        thresholds[alpha] = None\n",
    "\n",
    "# Create plots\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Empirical distribution (linear scale)\n",
    "bins = np.arange(1, max(cluster_sizes) + 2) - 0.5\n",
    "counts, _ = np.histogram(cluster_sizes, bins=bins, density=True)\n",
    "bin_centers = np.arange(1, max(cluster_sizes) + 1)\n",
    "\n",
    "ax1.bar(bin_centers, counts, alpha=0.7, edgecolor='black', width=0.8)\n",
    "ax1.set_xlabel('Cluster Size')\n",
    "ax1.set_ylabel('Probability Density')\n",
    "ax1.set_title(f'Empirical Null Distribution (N={len(cluster_sizes)})')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add threshold lines\n",
    "colors = ['orange', 'red', 'purple', 'black']\n",
    "for i, (alpha, threshold) in enumerate(thresholds.items()):\n",
    "    if threshold is not None:\n",
    "        ax1.axvline(threshold, color=colors[i], linestyle='--', \n",
    "                   label=f'{(1-alpha)*100:.1f}% threshold: {threshold}')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Empirical distribution (log scale)\n",
    "ax2.bar(bin_centers, counts, alpha=0.7, edgecolor='black', width=0.8)\n",
    "ax2.set_xlabel('Cluster Size')\n",
    "ax2.set_ylabel('Probability Density')\n",
    "ax2.set_title('Empirical Null Distribution (Log Scale)')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add threshold lines\n",
    "for i, (alpha, threshold) in enumerate(thresholds.items()):\n",
    "    if threshold is not None:\n",
    "        ax2.axvline(threshold, color=colors[i], linestyle='--', \n",
    "                   label=f'{(1-alpha)*100:.1f}% threshold: {threshold}')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Survival function (linear scale)\n",
    "ax3.plot(size_range, survival_probs, 'bo-', markersize=4, linewidth=2)\n",
    "ax3.set_xlabel('Cluster Size')\n",
    "ax3.set_ylabel('P(Size ≥ n)')\n",
    "ax3.set_title('Empirical Survival Function')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add significance levels\n",
    "for i, alpha in enumerate(alpha_levels):\n",
    "    ax3.axhline(alpha, color=colors[i], linestyle='--', alpha=0.7,\n",
    "               label=f'α = {alpha}')\n",
    "    if thresholds[alpha] is not None:\n",
    "        ax3.plot(thresholds[alpha], alpha, 'o', color=colors[i], markersize=8)\n",
    "\n",
    "ax3.legend()\n",
    "ax3.set_ylim(0, 1)\n",
    "\n",
    "# 4. Survival function (log scale)\n",
    "ax4.plot(size_range, survival_probs, 'bo-', markersize=4, linewidth=2)\n",
    "ax4.set_xlabel('Cluster Size')\n",
    "ax4.set_ylabel('P(Size ≥ n)')\n",
    "ax4.set_title('Empirical Survival Function (Log Scale)')\n",
    "ax4.set_yscale('log')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Add significance levels\n",
    "for i, alpha in enumerate(alpha_levels):\n",
    "    ax4.axhline(alpha, color=colors[i], linestyle='--', alpha=0.7,\n",
    "               label=f'α = {alpha}')\n",
    "    if thresholds[alpha] is not None:\n",
    "        ax4.plot(thresholds[alpha], alpha, 'o', color=colors[i], markersize=8)\n",
    "\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Null Distribution Summary:\")\n",
    "print(f\"Total clusters: {len(cluster_sizes)}\")\n",
    "print(f\"Size range: {min(cluster_sizes)} - {max(cluster_sizes)}\")\n",
    "print(f\"Mean size: {np.mean(cluster_sizes):.2f}\")\n",
    "print(f\"Median size: {np.median(cluster_sizes):.2f}\")\n",
    "\n",
    "print(f\"\\nSignificance Thresholds:\")\n",
    "for alpha in alpha_levels:\n",
    "    if thresholds[alpha] is not None:\n",
    "        confidence = (1 - alpha) * 100\n",
    "        n_expected = empirical_survival(thresholds[alpha]) * len(cluster_sizes)\n",
    "        print(f\"{confidence:5.1f}% confidence (α={alpha:5.3f}): size ≥ {thresholds[alpha]:2d} \"\n",
    "              f\"(p={empirical_survival(thresholds[alpha]):.4f}, {n_expected:.1f} expected)\")\n",
    "    else:\n",
    "        print(f\"{(1-alpha)*100:5.1f}% confidence (α={alpha:5.3f}): No threshold found\")\n",
    "\n",
    "# Detailed tail probabilities\n",
    "print(f\"\\nTail Probabilities:\")\n",
    "for size in range(10, min(30, max(cluster_sizes) + 1), 5):\n",
    "    prob = empirical_survival(size)\n",
    "    expected = prob * len(cluster_sizes)\n",
    "    print(f\"P(size ≥ {size:2d}) = {prob:.4f} ({expected:.1f} expected)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce499611-e7b7-4a89-a8f7-ba513f348b56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manticore-c7",
   "language": "python",
   "name": "manticore-c7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
